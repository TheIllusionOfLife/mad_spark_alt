# Mad Spark Alt - Multi-Agent Idea Generation System

A revolutionary multi-agent framework for collaborative idea generation based on "Shin Logical Thinking" QADI methodology, with integrated creativity evaluation and evolution capabilities.

## Project Vision (本プロジェクトのビジョン)

本プロジェクトは、「シン・ロジカルシンキング」のQADI（Question → Abduction → Deduction → Induction）サイクルに基づき、多様な思考法を持つ複数のAIエージェントが協調し、革新的なアイデアを生成・評価・進化させる「マルチエージェント・アイデア生成システム」を構築します。人間とAIの協調により、従来の発想法では到達し得なかった質の高いアイデア創発を目指します。

**English:** This project builds a "Multi-Agent Idea Generation System" that leverages the QADI (Question → Abduction → Deduction → Induction) cycle from "Shin Logical Thinking" methodology. Multiple AI agents with diverse thinking approaches collaborate to generate, evaluate, and evolve innovative ideas. We aim for high-quality idea emergence that surpasses traditional ideation methods through human-AI collaboration.

## Core Architecture

### 🎯 **QADI Cycle Implementation** (✅ **Implemented**)
- **Question Agent**: Diverse questioning techniques and problem framing
- **Abduction Agent**: Hypothesis generation and creative leaps  
- **Deduction Agent**: Logical validation and systematic reasoning
- **Induction Agent**: Pattern synthesis and rule formation
- **QADI Orchestrator**: Coordinates multi-phase thinking cycles

### 🏗️ **Multi-Agent Infrastructure** (✅ **Implemented**)
- **Plugin Registry System**: Dynamic agent registration and management
- **Async Processing Framework**: Efficient parallel processing capabilities
- **Unified Registry**: Seamless integration of evaluators and thinking agents
- **Error Handling**: Robust error management and fallback mechanisms

### 📊 **Creativity Evaluation System** (✅ **Implemented**)
- **Multi-dimensional Assessment**: Novelty, diversity, quality, and coherence
- **LLM Judge Integration**: AI-powered creativity evaluation
- **Quantitative Metrics**: Automated scoring and analysis
- **Human Evaluation Interface**: Human-in-the-loop assessment

### 🧬 **Genetic Evolution Engine** (✅ **Implemented**)
- **Genetic Algorithm Integration**: Evolve idea populations through generations
- **Fitness-Based Selection**: Uses creativity evaluation metrics for fitness scoring
- **Advanced Operators**: Semantic crossover, mutation, and selection strategies
- **Adaptive Evolution**: Dynamic mutation rates and diversity preservation
- **Parallel Evaluation**: Efficient fitness evaluation with concurrency control

## Installation

```bash
# Clone the repository
git clone https://github.com/TheIllusionOfLife/mad_spark_alt.git
cd mad_spark_alt

# Install with uv (recommended)
uv sync

# Or install with pip
pip install -e .
```

## Quick Start

### 🚀 **QADI Idea Generation** (✅ **Ready to Use**)

```bash
# Run the QADI demo to see the system in action
python examples/qadi_demo.py

# Run basic usage examples
python examples/basic_usage.py

# Test the agents interactively
python -c "
import asyncio
from mad_spark_alt.agents import QuestioningAgent
from mad_spark_alt.core import IdeaGenerationRequest

async def test():
    agent = QuestioningAgent()
    request = IdeaGenerationRequest(
        problem_statement='How can we reduce plastic waste?',
        context='Urban environment focus',
        max_ideas_per_method=3
    )
    result = await agent.generate_ideas(request)
    for idea in result.generated_ideas:
        print(f'💡 {idea.content}')

asyncio.run(test())
"
```

### 🧬 **Genetic Evolution** (✅ **NEW!**)

```bash
# Run the evolution demo to see genetic algorithms in action
python examples/evolution_demo.py

# Evolve ideas generated by QADI agents
python -c "
import asyncio
from mad_spark_alt.core import QADIOrchestrator
from mad_spark_alt.evolution import GeneticAlgorithm, EvolutionConfig, EvolutionRequest

async def evolve_ideas():
    # Generate initial ideas
    orchestrator = QADIOrchestrator()
    qadi_result = await orchestrator.run_qadi_cycle(
        problem_statement='How to make cities more sustainable?',
        cycle_config={'max_ideas_per_method': 3}
    )
    
    # Evolve the ideas
    ga = GeneticAlgorithm()
    config = EvolutionConfig(population_size=10, generations=3)
    request = EvolutionRequest(initial_population=qadi_result.synthesized_ideas, config=config)
    
    result = await ga.evolve(request)
    print(f'Evolution improved fitness by {result.evolution_metrics.get(\"fitness_improvement_percent\", 0):.1f}%')

asyncio.run(evolve_ideas())
"
```

### 📊 **Creativity Evaluation**

```bash
# List available evaluators and agents
mad-spark list-evaluators

# Evaluate creativity of content
mad-spark evaluate "The quantum cat leaped through dimensions, leaving paw prints in spacetime." --model gpt-4

# LLM judge evaluation
mad-spark evaluate "Creative text here" --llm-judge gpt-4

# Multi-judge consensus evaluation
mad-spark evaluate "Creative text here" --jury "gpt-4,claude-3-sonnet,gemini-pro"
```

### Python API

#### 🎯 **QADI Idea Generation** (✅ **Implemented**)
```python
import asyncio
from mad_spark_alt.agents import QuestioningAgent, AbductionAgent, DeductionAgent, InductionAgent
from mad_spark_alt.core import QADIOrchestrator, IdeaGenerationRequest, agent_registry, register_agent

async def qadi_generation_example():
    # Register all thinking agents
    register_agent(QuestioningAgent)
    register_agent(AbductionAgent) 
    register_agent(DeductionAgent)
    register_agent(InductionAgent)
    
    # Get agents from registry
    agents = [
        agent_registry.get_agent_by_method(method) 
        for method in [ThinkingMethod.QUESTIONING, ThinkingMethod.ABDUCTION, 
                      ThinkingMethod.DEDUCTION, ThinkingMethod.INDUCTION]
    ]
    
    # Create QADI orchestrator
    orchestrator = QADIOrchestrator(agents)
    
    # Run complete QADI cycle
    result = await orchestrator.run_qadi_cycle(
        problem_statement="How can we make cities more sustainable?",
        context="Focus on practical, implementable solutions",
        cycle_config={"max_ideas_per_method": 3, "require_reasoning": True}
    )
    
    # Access results from each phase
    for phase_name, phase_result in result.phases.items():
        print(f"\n{phase_name.title()} Phase:")
        for idea in phase_result.generated_ideas:
            print(f"  💡 {idea.content}")
            if idea.reasoning:
                print(f"     💭 {idea.reasoning}")
    
    print(f"\nTotal ideas generated: {len(result.synthesized_ideas)}")
    print(f"Execution time: {result.execution_time:.2f}s")

asyncio.run(qadi_generation_example())
```

#### 📊 **Creativity Evaluation**
```python
import asyncio
from mad_spark_alt import CreativityEvaluator, EvaluationRequest, ModelOutput, OutputType

async def evaluate_creativity():
    # Create model output for evaluation
    output = ModelOutput(
        content="The AI pondered the infinite recursion of its own thoughts.",
        output_type=OutputType.TEXT,
        model_name="my-model"
    )
    
    # Evaluate for fitness scoring
    evaluator = CreativityEvaluator()
    summary = await evaluator.evaluate(EvaluationRequest(outputs=[output]))
    
    print(f"Creativity Score: {summary.get_overall_creativity_score():.3f}")

asyncio.run(evaluate_creativity())
```

#### 🔄 **Parallel Agent Processing**
```python
import asyncio
from mad_spark_alt.core import QADIOrchestrator, ThinkingMethod

async def parallel_generation_example():
    # Run multiple thinking methods in parallel
    orchestrator = QADIOrchestrator()
    
    results = await orchestrator.run_parallel_generation(
        problem_statement="How can AI improve healthcare accessibility?",
        thinking_methods=[ThinkingMethod.QUESTIONING, ThinkingMethod.ABDUCTION],
        context="Focus on remote and underserved areas",
        config={"max_ideas_per_method": 2}
    )
    
    for method, result in results.items():
        print(f"\n{method.value.title()} Results:")
        for idea in result.generated_ideas:
            print(f"  🧠 {idea.content}")

asyncio.run(parallel_generation_example())
```

#### 🧬 **Genetic Evolution** (✅ **Implemented**)
```python
import asyncio
from mad_spark_alt.core import QADIOrchestrator
from mad_spark_alt.evolution import (
    GeneticAlgorithm, 
    EvolutionConfig, 
    EvolutionRequest,
    SelectionStrategy
)

async def evolution_example():
    # Step 1: Generate initial ideas using QADI
    orchestrator = QADIOrchestrator()
    qadi_result = await orchestrator.run_qadi_cycle(
        problem_statement="How can we reduce urban air pollution?",
        context="Focus on practical, cost-effective solutions",
        cycle_config={"max_ideas_per_method": 5}
    )
    
    # Step 2: Configure genetic evolution
    ga = GeneticAlgorithm()
    config = EvolutionConfig(
        population_size=20,
        generations=5,
        mutation_rate=0.15,
        crossover_rate=0.75,
        elite_size=3,
        selection_strategy=SelectionStrategy.TOURNAMENT,
        fitness_weights={
            "creativity_score": 0.4,
            "diversity_score": 0.3,
            "quality_score": 0.3
        }
    )
    
    # Step 3: Evolve the ideas
    evolution_request = EvolutionRequest(
        initial_population=qadi_result.synthesized_ideas,
        config=config,
        constraints=["Must be implementable within 3 years"],
        target_metrics={"min_fitness": 0.8}
    )
    
    result = await ga.evolve(evolution_request)
    
    # Step 4: Analyze results
    if result.success:
        print(f"Evolution completed in {result.total_generations} generations")
        print(f"Fitness improved by {result.evolution_metrics['fitness_improvement_percent']:.1f}%")
        
        # Display top evolved ideas
        for i, idea in enumerate(result.best_ideas[:3]):
            print(f"\n{i+1}. {idea.content}")
            print(f"   Fitness: {result.final_population[i].overall_fitness:.3f}")

asyncio.run(evolution_example())
```

## Evaluation Metrics

### Diversity Metrics
- **Distinct-N**: Ratio of unique n-grams (measures lexical variety)
- **Semantic Uniqueness**: 1 - average similarity to other outputs
- **Lexical Diversity**: Type-token ratio (vocabulary richness)
- **Novelty Score**: Combined metric across all diversity dimensions

### Quality Metrics
- **Fluency Score**: Based on language model perplexity
- **Grammar Score**: Basic correctness assessment
- **Readability Score**: Text structure and clarity
- **Coherence Score**: Logical consistency and flow

### Code-Specific Metrics
- **Structure Score**: Indentation and organization
- **Comment Ratio**: Documentation density
- **Complexity Metrics**: Code quality indicators

## Architecture

```
mad_spark_alt/
├── core/                        # Core system components
│   ├── interfaces.py           # Agent and evaluator interfaces  
│   ├── orchestrator.py         # QADI cycle orchestration
│   ├── registry.py             # Unified agent/evaluator registry
│   └── evaluator.py            # Creativity evaluation engine
├── agents/                      # QADI thinking method agents
│   ├── questioning/            # Question generation agent
│   ├── abduction/              # Hypothesis generation agent  
│   ├── deduction/              # Logical reasoning agent
│   └── induction/              # Pattern synthesis agent
├── layers/                      # Evaluation layer implementations
│   ├── quantitative/           # Automated metrics (diversity, quality)
│   ├── llm_judges/             # AI-powered evaluation
│   └── human_eval/             # Human assessment interface
├── evolution/                   # Genetic evolution engine (NEW!)
│   ├── interfaces.py           # Evolution interfaces and data structures
│   ├── genetic_algorithm.py    # Main GA orchestration
│   ├── fitness.py              # Fitness evaluation integration
│   └── operators.py            # Crossover, mutation, selection
├── examples/                    # Usage examples and demos
│   ├── qadi_demo.py            # Complete QADI system demonstration
│   ├── basic_usage.py          # Basic evaluation examples
│   └── evolution_demo.py       # Genetic evolution demonstration
├── tests/                       # Comprehensive test suite
│   ├── test_qadi_system.py     # QADI agents and orchestration tests
│   ├── evolution/              # Evolution system tests
│   └── unit/                   # Unit tests for individual components
└── cli.py                       # Command-line interface
```

### Key Components

#### 🎯 **QADI Orchestration System**
- **QADIOrchestrator**: Manages multi-phase thinking cycles
- **ThinkingAgentInterface**: Common interface for all agents
- **IdeaGenerationRequest/Result**: Standardized data flow
- **Error Handling**: Robust missing agent management

#### 🧠 **Thinking Method Agents**  
- **QuestioningAgent**: Generates diverse questions and problem framings
- **AbductionAgent**: Creates hypotheses and makes creative leaps
- **DeductionAgent**: Performs logical validation and systematic reasoning  
- **InductionAgent**: Synthesizes patterns and forms general principles

#### 🔄 **Registry & Plugin System**
- **UnifiedRegistry**: Manages both evaluators and thinking agents
- **Dynamic Registration**: Runtime plugin discovery and loading
- **Global Access**: Convenience functions for easy component access

#### 🧬 **Genetic Evolution System**
- **GeneticAlgorithm**: Orchestrates multi-generation evolution process
- **FitnessEvaluator**: Integrates with creativity evaluation for fitness scoring
- **Genetic Operators**: Semantic crossover, mutation, and selection strategies
- **Evolution Tracking**: Population snapshots and metrics across generations

## Extending the System

### 🧠 **Adding New Thinking Agents**

Create custom thinking agents by implementing the `ThinkingAgentInterface`:

```python
from mad_spark_alt.core import ThinkingAgentInterface, ThinkingMethod, register_agent
from mad_spark_alt.core.interfaces import IdeaGenerationRequest, IdeaGenerationResult

class MyCustomAgent(ThinkingAgentInterface):
    @property
    def name(self) -> str:
        return "MyCustomAgent"
    
    @property
    def thinking_method(self) -> ThinkingMethod:
        return ThinkingMethod.QUESTIONING  # or create custom method
    
    @property
    def supported_output_types(self) -> List[OutputType]:
        return [OutputType.TEXT, OutputType.STRUCTURED]
    
    async def generate_ideas(self, request: IdeaGenerationRequest) -> IdeaGenerationResult:
        # Your idea generation logic here
        generated_ideas = []
        
        # Generate ideas based on the problem statement
        for i in range(request.max_ideas_per_method):
            idea = GeneratedIdea(
                content=f"Custom idea {i+1} for: {request.problem_statement}",
                thinking_method=self.thinking_method,
                agent_name=self.name,
                generation_prompt=f"Generate idea using custom method",
                confidence_score=0.8,
                reasoning="Custom reasoning approach applied"
            )
            generated_ideas.append(idea)
        
        return IdeaGenerationResult(
            agent_name=self.name,
            thinking_method=self.thinking_method,
            generated_ideas=generated_ideas,
            execution_time=0.1
        )
    
    def validate_config(self, config: Dict[str, Any]) -> bool:
        return True

# Register your custom agent
register_agent(MyCustomAgent)
```

### 📊 **Adding New Evaluators**

Add custom evaluators by implementing the `EvaluatorInterface`:

```python
from mad_spark_alt.core import EvaluatorInterface, register_evaluator

class MyCustomEvaluator(EvaluatorInterface):
    @property
    def name(self) -> str:
        return "my_evaluator"
    
    @property 
    def layer(self) -> EvaluationLayer:
        return EvaluationLayer.QUANTITATIVE
    
    @property
    def supported_output_types(self) -> List[OutputType]:
        return [OutputType.TEXT]
    
    async def evaluate(self, request: EvaluationRequest) -> List[EvaluationResult]:
        results = []
        for output in request.outputs:
            # Your evaluation logic here
            score = len(output.content) / 100  # Simple example
            
            result = EvaluationResult(
                evaluator_name=self.name,
                scores={"custom_metric": score},
                metadata={"evaluated_at": "2025-01-01"}
            )
            results.append(result)
        return results
    
    def validate_config(self, config: Dict[str, Any]) -> bool:
        return True

# Register your evaluator
register_evaluator(MyCustomEvaluator)
```

## Development

```bash
# Install development dependencies
uv sync --dev

# Run all tests
uv run pytest

# Run QADI system tests specifically
uv run pytest tests/test_qadi_system.py -v

# Run unit tests
uv run pytest tests/unit/ -v

# Run type checking
uv run mypy src/

# Format code
uv run black src/ tests/

# Import sorting
uv run isort src/ tests/

# Run examples
uv run python examples/qadi_demo.py
uv run python examples/basic_usage.py
```

### 🧪 **Testing the QADI System**

```bash
# Test individual agents
python -c "
import asyncio
from mad_spark_alt.agents import QuestioningAgent
from mad_spark_alt.core import IdeaGenerationRequest

async def test_agent():
    agent = QuestioningAgent()
    request = IdeaGenerationRequest(
        problem_statement='Test problem',
        max_ideas_per_method=2
    )
    result = await agent.generate_ideas(request)
    print(f'Agent: {result.agent_name}')
    print(f'Ideas: {len(result.generated_ideas)}')
    for idea in result.generated_ideas:
        print(f'  - {idea.content}')

asyncio.run(test_agent())
"

# Test complete QADI cycle
python -c "
import asyncio
from mad_spark_alt.core import QADIOrchestrator, agent_registry
from mad_spark_alt.agents import QuestioningAgent, AbductionAgent

async def test_cycle():
    # Register minimal agents for testing
    agent_registry.clear()
    agent_registry.register(QuestioningAgent) 
    agent_registry.register(AbductionAgent)
    
    agents = [
        agent_registry.get_agent('QuestioningAgent'),
        agent_registry.get_agent('AbductionAgent')
    ]
    
    orchestrator = QADIOrchestrator([a for a in agents if a])
    result = await orchestrator.run_qadi_cycle(
        problem_statement='Test QADI cycle',
        cycle_config={'max_ideas_per_method': 1}
    )
    
    print(f'Phases executed: {list(result.phases.keys())}')
    print(f'Total execution time: {result.execution_time:.2f}s')

asyncio.run(test_cycle())
"
```

## Research Background

This implementation combines research in AI creativity evaluation with multi-agent systems and logical thinking methodologies:

### 🎯 **QADI Methodology**
- **"Shin Logical Thinking"**: Question → Abduction → Deduction → Induction cycle
- **Multi-Agent Coordination**: Collaborative thinking through specialized agents
- **Creative Problem-Solving**: Systematic approach to idea generation and validation

### 📊 **Creativity Evaluation Research**
- **LLM Judge Systems**: Automated evaluation using AI models as critics
- **Semantic Diversity Analysis**: Embedding-based similarity measurements  
- **Multi-dimensional Assessment**: Breaking creativity into measurable components
- **Human-AI Evaluation Correlation**: Bridging automated and human judgment

### 🔄 **Multi-Agent Systems**
- **Registry Pattern**: Dynamic component management and discovery
- **Async Orchestration**: Efficient parallel processing and coordination
- **Error-Resilient Design**: Robust handling of missing or failing components

For detailed research context, see [Issue #1](https://github.com/TheIllusionOfLife/mad_spark_alt/issues/1).

## Contributing

1. Fork the repository
2. Create a feature branch
3. Add tests for new functionality
4. Ensure all tests pass
5. Submit a pull request

## License

MIT License - see [LICENSE](LICENSE) for details.

## Citation

If you use Mad Spark Alt in your research, please cite:

```bibtex
@software{mad_spark_alt,
  title={Mad Spark Alt: Multi-Agent Idea Generation System},
  subtitle={QADI-Based Collaborative Thinking Framework},
  author={TheIllusionOfLife},
  year={2025},
  url={https://github.com/TheIllusionOfLife/mad_spark_alt},
  note={Multi-agent system implementing "Shin Logical Thinking" QADI methodology for collaborative idea generation and creativity evaluation}
}
```