# CLAUDE.md

This file provides guidance to Claude Code (claude.ai/code) when working with code in this repository.

## Project Overview

**TRANSFORMATION COMPLETE:** Mad Spark Alt has evolved from an AI creativity evaluation system into a **Multi-Agent Idea Generation System** based on "Shin Logical Thinking" QADI methodology.

### Current State: QADI System Implementation âœ…
The system now provides a complete multi-agent framework with:
1. **QADI Cycle Orchestration** - Question â†’ Abduction â†’ Deduction â†’ Induction workflows âœ…
2. **Thinking Method Agents** - Specialized AI agents for different cognitive approaches âœ…  
3. **Unified Registry System** - Seamless management of evaluators and thinking agents âœ…
4. **Async Processing Framework** - Efficient multi-agent coordination âœ…
5. **Creativity Evaluation Engine** - Multi-dimensional assessment for fitness scoring âœ…

### Future Evolution: Genetic Algorithm Integration ðŸš§
The next phase will add:
1. **Genetic Evolution Engine** - Idea population evolution through genetic algorithms
2. **Human-AI Collaboration Interface** - Interactive ideation sessions and feedback loops
3. **Advanced Fitness Functions** - Sophisticated evaluation criteria for idea evolution

## Commands

### Development Environment
```bash
# Install dependencies (preferred)
uv sync

# Install with dev dependencies
uv sync --dev

# Alternative with pip
pip install -e .

# Run development server (if needed)
python main.py
```

### Testing & Quality
```bash
# Run tests
uv run pytest

# Run specific test file
uv run pytest tests/unit/test_llm_judges.py

# Run tests with coverage
uv run pytest --cov=src/mad_spark_alt --cov-report=html

# Type checking
uv run mypy src/

# Code formatting
uv run black src/ tests/

# Import sorting
uv run isort src/ tests/
```

### CLI Usage
```bash
# Main CLI entry point
mad-spark --help

# List available evaluators and agents
mad-spark list-evaluators

# List LLM judges
mad-spark list-judges

# Test LLM connections
mad-spark test-judges

# Evaluate single text
mad-spark evaluate "creative text" --model gpt-4

# LLM judge evaluation
mad-spark evaluate "text" --llm-judge gpt-4

# Multi-judge jury evaluation
mad-spark evaluate "text" --jury "gpt-4,claude-3-sonnet,gemini-pro"

# Pre-configured jury budgets
mad-spark evaluate "text" --jury-budget balanced
```

### QADI System Usage
```bash
# Run QADI demonstration
python examples/qadi_demo.py

# Run basic examples
python examples/basic_usage.py

# Test the complete system
uv run pytest tests/test_qadi_system.py -v

# Test individual agents
python -c "
import asyncio
from mad_spark_alt.agents import QuestioningAgent
from mad_spark_alt.core import IdeaGenerationRequest

async def test():
    agent = QuestioningAgent()
    request = IdeaGenerationRequest(
        problem_statement='How can we improve urban sustainability?',
        context='Focus on practical solutions',
        max_ideas_per_method=3
    )
    result = await agent.generate_ideas(request)
    for idea in result.generated_ideas:
        print(f'ðŸ’¡ {idea.content}')

asyncio.run(test())
"
```

### Environment Variables
LLM Judge functionality requires API keys:
```bash
export OPENAI_API_KEY="your-openai-key"
export ANTHROPIC_API_KEY="your-anthropic-key" 
export GOOGLE_API_KEY="your-google-key"
```

Current implementation primarily uses local evaluation methods.

## System Architecture (Current Implementation)

### âœ… Phase 1: Core Architecture (COMPLETED)
**Multi-Agent QADI Framework**

1. **Core Interfaces** (`core/interfaces.py`)
   - âœ… `ThinkingAgentInterface` - Common interface for all thinking agents
   - âœ… `IdeaGenerationRequest` - Standardized input for idea generation
   - âœ… `GeneratedIdea` - Rich idea representation with metadata
   - âœ… `ThinkingMethod` enum - QUESTIONING, ABDUCTION, DEDUCTION, INDUCTION
   - âœ… `IdeaGenerationResult` - Structured output from agents

2. **Unified Registry System** (`core/registry.py`)
   - âœ… `ThinkingAgentRegistry` - Agent management and discovery
   - âœ… `UnifiedRegistry` - Seamless evaluator and agent integration
   - âœ… Dynamic registration with convenience functions
   - âœ… Method-based agent retrieval and indexing

3. **QADI Orchestration Engine** (`core/orchestrator.py`)
   - âœ… `QADIOrchestrator` - Multi-phase cycle coordination
   - âœ… Sequential and parallel agent processing
   - âœ… Enhanced context building between phases
   - âœ… Robust error handling for missing agents
   - âœ… Idea synthesis and aggregation

### âœ… Phase 2: Thinking Method Agents (COMPLETED)
**"Shin Logical Thinking" Implementation**

1. **Questioning Agent** (`agents/questioning/`)
   - âœ… Diverse questioning strategies (clarifying, alternative, challenging, etc.)
   - âœ… Problem framing and assumption questioning
   - âœ… Context-aware question generation

2. **Abductive Agent** (`agents/abduction/`)
   - âœ… Hypothesis generation through creative leaps
   - âœ… Causal, analogical, and pattern-based reasoning
   - âœ… "What if" scenario exploration

3. **Deductive Agent** (`agents/deduction/`)
   - âœ… Logical validation and systematic reasoning
   - âœ… Structured consequence derivation
   - âœ… Constraint-based analysis

4. **Inductive Agent** (`agents/induction/`)
   - âœ… Pattern synthesis and rule formation
   - âœ… Generalization from specific observations
   - âœ… Meta-pattern recognition and insight extraction

### ðŸš§ Phase 3: Genetic Evolution Engine (PLANNED)
**Idea Population Evolution**

1. **Evolution Engine** (`evolution/genetic_algorithm.py`)
   - Genetic operators for idea crossover and mutation
   - Population management and selection strategies
   - Fitness evaluation using existing creativity metrics

2. **Human-AI Collaboration** (`collaboration/interface.py`)
   - Interactive ideation sessions
   - Real-time feedback integration
   - Collaborative idea refinement

## Implementation Architecture

### âœ… Current Structure (Fully Implemented QADI System)
```
src/mad_spark_alt/
â”œâ”€â”€ core/                        # âœ… Core system components
â”‚   â”œâ”€â”€ orchestrator.py         # âœ… QADI cycle coordination engine
â”‚   â”œâ”€â”€ interfaces.py           # âœ… Agent and evaluator interfaces
â”‚   â”œâ”€â”€ registry.py             # âœ… Unified agent/evaluator management
â”‚   â””â”€â”€ evaluator.py            # âœ… Creativity evaluation engine
â”œâ”€â”€ agents/                      # âœ… QADI thinking method agents
â”‚   â”œâ”€â”€ questioning/            # âœ… Question generation and framing
â”‚   â”‚   â””â”€â”€ agent.py           # âœ… QuestioningAgent implementation
â”‚   â”œâ”€â”€ abduction/              # âœ… Hypothesis generation and creative leaps
â”‚   â”‚   â””â”€â”€ agent.py           # âœ… AbductionAgent implementation
â”‚   â”œâ”€â”€ deduction/              # âœ… Logical validation and reasoning
â”‚   â”‚   â””â”€â”€ agent.py           # âœ… DeductionAgent implementation
â”‚   â””â”€â”€ induction/              # âœ… Pattern synthesis and generalization
â”‚       â””â”€â”€ agent.py           # âœ… InductionAgent implementation
â”œâ”€â”€ layers/                      # âœ… Evaluation infrastructure
â”‚   â”œâ”€â”€ quantitative/           # âœ… Automated metrics (diversity, quality)
â”‚   â”œâ”€â”€ llm_judges/             # âœ… AI-powered evaluation
â”‚   â””â”€â”€ human_eval/             # âœ… Human assessment interface
â”œâ”€â”€ examples/                    # âœ… Usage demonstrations
â”‚   â”œâ”€â”€ qadi_demo.py            # âœ… Complete QADI system demo
â”‚   â””â”€â”€ basic_usage.py          # âœ… Basic evaluation examples
â”œâ”€â”€ tests/                       # âœ… Comprehensive test suite
â”‚   â”œâ”€â”€ test_qadi_system.py     # âœ… QADI agents and orchestration tests
â”‚   â””â”€â”€ unit/                   # âœ… Unit tests for components
â””â”€â”€ cli.py                       # âœ… Command-line interface
```

### ðŸš§ Future Enhancements (Genetic Evolution)
```
src/mad_spark_alt/
â”œâ”€â”€ evolution/                   # ðŸš§ Genetic algorithm implementation
â”‚   â”œâ”€â”€ genetic_algorithm.py    # ðŸš§ Population evolution engine
â”‚   â”œâ”€â”€ fitness.py              # ðŸš§ Idea fitness evaluation
â”‚   â””â”€â”€ operators.py            # ðŸš§ Crossover, mutation, selection
â”œâ”€â”€ collaboration/               # ðŸš§ Human-AI interaction
â”‚   â”œâ”€â”€ interface.py            # ðŸš§ Interactive ideation sessions
â”‚   â””â”€â”€ feedback.py             # ðŸš§ Human feedback integration
â””â”€â”€ web/                         # ðŸš§ Web interface (optional)
    â”œâ”€â”€ api.py                  # ðŸš§ REST API for remote access
    â””â”€â”€ dashboard.py            # ðŸš§ Real-time monitoring dashboard
```

### Key Classes

**Core Interfaces** (`core/interfaces.py`):
- `ThinkingAgentInterface`: Abstract base for all thinking agents âœ…
- `EvaluatorInterface`: Abstract base for all evaluators âœ…
- `IdeaGenerationRequest`: Input data structure for idea generation âœ…
- `IdeaGenerationResult`: Output data structure from agents âœ…
- `GeneratedIdea`: Rich idea representation with metadata âœ…
- `ThinkingMethod`: Enum (QUESTIONING, ABDUCTION, DEDUCTION, INDUCTION) âœ…
- `EvaluationRequest`: Input data structure for evaluation âœ…
- `EvaluationResult`: Output data structure from evaluators âœ…
- `ModelOutput`: Represents AI-generated content to evaluate âœ…

**QADI Orchestration** (`core/orchestrator.py`):
- `QADIOrchestrator`: Coordinates multi-phase thinking cycles âœ…
- `QADICycleResult`: Complete cycle result with phase breakdowns âœ…
- Handles sequential and parallel agent processing âœ…
- Enhanced context building between phases âœ…

**Registry System** (`core/registry.py`):
- `ThinkingAgentRegistry`: Dynamic agent registration and management âœ…
- `EvaluatorRegistry`: Dynamic evaluator registration âœ… 
- `UnifiedRegistry`: Seamless integration of both systems âœ…
- Method-based agent retrieval and discovery âœ…

**Creativity Evaluation** (`core/evaluator.py`):
- `CreativityEvaluator`: Coordinates evaluation across all layers âœ…
- Handles async execution, result aggregation, scoring âœ…

### Layer Implementations

**Quantitative Layer**:
- `DiversityEvaluator`: Distinct-N, semantic uniqueness, lexical diversity
- `QualityEvaluator`: Fluency, grammar, readability, coherence

**LLM Judge Layer**:
- `CreativityLLMJudge`: Single AI model evaluation
- `CreativityJury`: Multi-judge consensus with disagreement detection
- Supports GPT-4, Claude-3, Gemini models
- Cost tracking and budget management

## Development Patterns

### Adding New Evaluators
Implement `EvaluatorInterface` and register:

```python
from typing import Any, Dict, List
from mad_spark_alt.core import EvaluatorInterface, register_evaluator
from mad_spark_alt.core.interfaces import EvaluationLayer, EvaluationRequest, EvaluationResult, OutputType

class MyEvaluator(EvaluatorInterface):
    @property
    def name(self) -> str:
        return "my_evaluator"
    
    @property
    def layer(self) -> EvaluationLayer:
        return EvaluationLayer.QUANTITATIVE
    
    @property
    def supported_output_types(self) -> List[OutputType]:
        return [OutputType.TEXT]
    
    async def evaluate(self, request: EvaluationRequest) -> List[EvaluationResult]:
        # Implementation here
        pass
    
    def validate_config(self, config: Dict[str, Any]) -> bool:
        return True

register_evaluator(MyEvaluator)
```

### Async Pattern
All evaluators use async/await for I/O operations:
- LLM API calls are concurrent where possible
- Batch processing for efficiency
- Proper error handling and timeouts

### Data Flow
1. `EvaluationRequest` created with `ModelOutput` list
2. `CreativityEvaluator.evaluate()` routes to appropriate layers
3. Each evaluator returns `EvaluationResult` list  
4. Results aggregated into `EvaluationSummary`
5. Overall creativity score calculated

### Error Handling
- Graceful fallbacks for missing API keys
- Individual evaluator failures don't crash entire evaluation
- Structured error responses with context

## Testing Strategy

### Test Structure
```
tests/
â”œâ”€â”€ unit/                   # Unit tests for individual components
â”‚   â”œâ”€â”€ test_core.py       # Core functionality tests
â”‚   â””â”€â”€ test_llm_judges.py # LLM judge specific tests
â””â”€â”€ integration/           # End-to-end integration tests
```

### Mock Strategy
- Mock external API calls to avoid costs during testing
- Test data includes various creativity dimensions
- Separate tests for different output types (text, code)

### Test Execution
Run tests without external dependencies by default. Use environment variables to enable real API testing during development.

### Quick Development Verification
```bash
# Verify installation and basic functionality
mad-spark --help
mad-spark list-evaluators

# Test basic evaluation
mad-spark evaluate "The AI dreamed of electric sheep in quantum meadows."

# Test core functionality without API keys
uv run pytest tests/unit/test_core.py -v

# Run all tests
uv run pytest

# Check registry state (useful for debugging)
python -c "from mad_spark_alt.core import registry; print([e.name for e in registry.get_evaluators()])"
```

## Important Implementation Notes

### Cost Management
LLM judges track token usage and costs. Budget-aware jury configurations prevent runaway expenses.

### Multi-Judge Consensus
The jury system uses voting mechanisms to handle disagreements between AI judges, improving evaluation reliability.

### Extensibility
The plugin architecture allows easy addition of new evaluation methods without modifying core code.

### Performance
Async execution enables parallel evaluation across multiple AI services, significantly reducing total evaluation time.