# CLAUDE.md

This file provides guidance to Claude Code (claude.ai/code) when working with code in this repository.

## Project Overview

**INTELLIGENT SYSTEM COMPLETE:** Mad Spark Alt is a **sophisticated Multi-Agent Idea Generation System** powered by Large Language Models and based on "Shin Logical Thinking" QADI methodology.

### Current State: Advanced LLM-Powered System ‚úÖ
The system provides an intelligent multi-agent framework with:
1. **Smart QADI Orchestration** - AI-powered Question ‚Üí Abduction ‚Üí Deduction ‚Üí Induction workflows ‚úÖ
2. **LLM-Powered Agents** - Sophisticated AI agents using OpenAI, Anthropic, and Google APIs ‚úÖ  
3. **Intelligent Agent Registry** - Automatic LLM preference with graceful template fallback ‚úÖ
4. **Cost-Aware Processing** - Real-time LLM cost tracking and optimization ‚úÖ
5. **Multi-Provider Support** - Seamless integration across multiple LLM providers ‚úÖ
6. **Robust Fallback System** - Automatic degradation to template agents when needed ‚úÖ
7. **Creativity Evaluation Engine** - Multi-dimensional assessment for idea fitness scoring ‚úÖ

### Next Evolution: Advanced AI Integration üöß
Future enhancements will include:
1. **Genetic Algorithm Engine** - Evolution of LLM-generated ideas through fitness-based selection
2. **Context-Aware Processing** - Domain knowledge integration and specialized reasoning
3. **Human-AI Collaboration** - Interactive ideation with intelligent agent assistance
4. **Advanced Analytics** - Multi-dimensional creativity and feasibility assessment

## Commands

### Development Environment
```bash
# Install dependencies (preferred)
uv sync

# Install with dev dependencies
uv sync --dev

# Alternative with pip
pip install -e .

# Run development server (if needed)
python main.py
```

### Testing & Quality
```bash
# Run tests
uv run pytest

# Run specific test file
uv run pytest tests/unit/test_llm_judges.py

# Run tests with coverage
uv run pytest --cov=src/mad_spark_alt --cov-report=html

# Type checking
uv run mypy src/

# Code formatting
uv run black src/ tests/

# Import sorting
uv run isort src/ tests/
```

### CLI Usage
**IMPORTANT**: CLI commands must be run with `uv run` prefix due to package installation method.

```bash
# Install package first (required for CLI access)
uv pip install -e .

# Main CLI entry point
uv run mad-spark --help

# List available evaluators
uv run mad-spark list-evaluators

# Evaluate single text for creativity
uv run mad-spark evaluate "The AI dreamed of electric sheep in quantum meadows."

# Batch evaluate multiple files
uv run mad-spark batch-evaluate file1.txt file2.txt file3.txt

# Compare creativity of multiple responses
uv run mad-spark compare "idea1" "idea2" "idea3"
```

**Note**: The CLI currently supports evaluation functionality. QADI generation is available through Python API only.

### Smart QADI System Usage (Python API)
**IMPORTANT**: QADI generation now uses intelligent LLM-powered agents with automatic fallback.

```bash
# Set API keys for LLM-powered generation (recommended)
export OPENAI_API_KEY="your-openai-key"
export ANTHROPIC_API_KEY="your-anthropic-key" 
export GOOGLE_API_KEY="your-google-key"

# Run Smart QADI demonstration (shows LLM vs template agents)
uv run python examples/qadi_demo.py

# Run LLM showcase demo (requires API keys)
uv run python examples/llm_showcase_demo.py

# Run LLM-specific agent demos
uv run python examples/llm_questioning_demo.py
uv run python examples/llm_abductive_demo.py

# Test the intelligent system
uv run pytest tests/test_qadi_system.py -v

# Test smart agent setup
uv run python -c "
import asyncio
from mad_spark_alt.core import SmartQADIOrchestrator

async def test():
    orchestrator = SmartQADIOrchestrator()
    result = await orchestrator.run_qadi_cycle(
        problem_statement='How can we improve urban sustainability?',
        context='Focus on practical, innovative solutions',
        cycle_config={'max_ideas_per_method': 3}
    )
    print(f'Generated {len(result.synthesized_ideas)} ideas')
    print(f'Agent types: {set(result.agent_types.values())}')
    if result.llm_cost > 0:
        print(f'LLM cost: \${result.llm_cost:.4f}')
    for phase, phase_result in result.phases.items():
        print(f'{phase.upper()}: {len(phase_result.generated_ideas)} ideas')

asyncio.run(test())
"
```

**Expected Output**: The Smart QADI system produces sophisticated AI-powered ideas:
- **With API Keys**: Context-aware, intelligent reasoning with detailed explanations
- **Without API Keys**: Graceful fallback to template-based generation
- **Cost Tracking**: Real-time monitoring of LLM usage and costs
- **Agent Types**: Clear indication of LLM vs template agent usage

This is intelligent Phase 2+ behavior - AI-powered reasoning with robust fallbacks.

### Environment Variables
LLM Judge functionality requires API keys:
```bash
export OPENAI_API_KEY="your-openai-key"
export ANTHROPIC_API_KEY="your-anthropic-key" 
export GOOGLE_API_KEY="your-google-key"
```

Current implementation primarily uses local evaluation methods.

## Current System State & Expectations

### ‚úÖ What Works Now (Phase 2+ - LLM-Powered)
- **Smart QADI Orchestration**: All 4 phases use intelligent LLM reasoning with fallback
- **LLM-Powered Generation**: Agents use sophisticated AI reasoning from OpenAI, Anthropic, Google
- **Intelligent Agent Selection**: Automatic preference for LLM agents with graceful template fallback
- **Cost-Aware Processing**: Real-time LLM usage tracking and cost optimization
- **System Integration**: Smart registry, orchestrator, and agents work seamlessly
- **Evaluation Engine**: Multi-dimensional creativity assessment with quantitative metrics
- **CLI Interface**: Basic evaluation commands (generation via Python API)

### üîç Expected Output Characteristics
The Smart QADI system produces **intelligent AI-powered results**:

**With LLM Agents**:
- **Questioning**: "What stakeholder perspectives haven't been considered in urban sustainability? How might economic incentives conflict with environmental goals?"
- **Abduction**: "This problem might stem from systemic misalignment between short-term economic pressures and long-term sustainability benefits, suggesting we need innovative financing mechanisms..."
- **Deduction**: "If we implement carbon pricing mechanisms, then logically we must also provide transition support for affected industries to prevent economic disruption while achieving environmental goals."
- **Induction**: "Analyzing patterns across successful sustainability initiatives reveals that effective solutions typically integrate economic incentives, stakeholder engagement, and measurable impact metrics."

**This represents sophisticated AI reasoning** - context awareness, stakeholder analysis, and domain-specific insights.

### üöß What's Coming Next (Future Phases)
- **Genetic Algorithm Engine**: Evolution of LLM-generated ideas through fitness-based selection
- **Context-Aware Processing**: Domain knowledge integration and specialized reasoning strategies
- **Human-AI Collaboration**: Interactive ideation sessions with intelligent agent assistance
- **Advanced Analytics**: Multi-dimensional creativity and feasibility assessment frameworks

### üìã Key Files for New Sessions
- **USER_GUIDE.md**: Complete user experience guide with LLM and template agent examples
- **examples/qadi_demo.py**: Smart QADI demonstration with LLM vs template comparison
- **examples/llm_showcase_demo.py**: Advanced LLM agent capabilities showcase
- **src/mad_spark_alt/core/smart_registry.py**: Intelligent agent registration system
- **src/mad_spark_alt/core/smart_orchestrator.py**: Smart QADI orchestration with LLM preference
- **src/mad_spark_alt/agents/*/llm_agent.py**: LLM-powered agent implementations
- **tests/test_qadi_system.py**: Verification that all components work correctly

## System Architecture (Current Implementation)

### ‚úÖ Phase 2+: Advanced LLM-Powered Architecture (COMPLETED)
**Intelligent Multi-Agent QADI Framework**

1. **Core Interfaces** (`core/interfaces.py`)
   - ‚úÖ `ThinkingAgentInterface` - Common interface for all thinking agents
   - ‚úÖ `IdeaGenerationRequest` - Standardized input for idea generation
   - ‚úÖ `GeneratedIdea` - Rich idea representation with metadata
   - ‚úÖ `ThinkingMethod` enum - QUESTIONING, ABDUCTION, DEDUCTION, INDUCTION
   - ‚úÖ `IdeaGenerationResult` - Structured output from agents

2. **Unified Registry System** (`core/registry.py`)
   - ‚úÖ `ThinkingAgentRegistry` - Agent management and discovery
   - ‚úÖ `UnifiedRegistry` - Seamless evaluator and agent integration
   - ‚úÖ Dynamic registration with convenience functions
   - ‚úÖ Method-based agent retrieval and indexing

3. **QADI Orchestration Engine** (`core/orchestrator.py`)
   - ‚úÖ `QADIOrchestrator` - Multi-phase cycle coordination
   - ‚úÖ Sequential and parallel agent processing
   - ‚úÖ Enhanced context building between phases
   - ‚úÖ Robust error handling for missing agents
   - ‚úÖ Idea synthesis and aggregation

### ‚úÖ Phase 2: Thinking Method Agents (COMPLETED)
**"Shin Logical Thinking" Implementation**

1. **Questioning Agent** (`agents/questioning/`)
   - ‚úÖ Diverse questioning strategies (clarifying, alternative, challenging, etc.)
   - ‚úÖ Problem framing and assumption questioning
   - ‚úÖ Context-aware question generation

2. **Abductive Agent** (`agents/abduction/`)
   - ‚úÖ Hypothesis generation through creative leaps
   - ‚úÖ Causal, analogical, and pattern-based reasoning
   - ‚úÖ "What if" scenario exploration

3. **Deductive Agent** (`agents/deduction/`)
   - ‚úÖ Logical validation and systematic reasoning
   - ‚úÖ Structured consequence derivation
   - ‚úÖ Constraint-based analysis

4. **Inductive Agent** (`agents/induction/`)
   - ‚úÖ Pattern synthesis and rule formation
   - ‚úÖ Generalization from specific observations
   - ‚úÖ Meta-pattern recognition and insight extraction

### üöß Phase 3: Genetic Evolution Engine (PLANNED)
**Idea Population Evolution**

1. **Evolution Engine** (`evolution/genetic_algorithm.py`)
   - Genetic operators for idea crossover and mutation
   - Population management and selection strategies
   - Fitness evaluation using existing creativity metrics

2. **Human-AI Collaboration** (`collaboration/interface.py`)
   - Interactive ideation sessions
   - Real-time feedback integration
   - Collaborative idea refinement

## Implementation Architecture

### ‚úÖ Current Structure (Fully Implemented QADI System)
```
src/mad_spark_alt/
‚îú‚îÄ‚îÄ core/                        # ‚úÖ Core system components
‚îÇ   ‚îú‚îÄ‚îÄ orchestrator.py         # ‚úÖ QADI cycle coordination engine
‚îÇ   ‚îú‚îÄ‚îÄ interfaces.py           # ‚úÖ Agent and evaluator interfaces
‚îÇ   ‚îú‚îÄ‚îÄ registry.py             # ‚úÖ Unified agent/evaluator management
‚îÇ   ‚îî‚îÄ‚îÄ evaluator.py            # ‚úÖ Creativity evaluation engine
‚îú‚îÄ‚îÄ agents/                      # ‚úÖ QADI thinking method agents
‚îÇ   ‚îú‚îÄ‚îÄ questioning/            # ‚úÖ Question generation and framing
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ agent.py           # ‚úÖ QuestioningAgent implementation
‚îÇ   ‚îú‚îÄ‚îÄ abduction/              # ‚úÖ Hypothesis generation and creative leaps
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ agent.py           # ‚úÖ AbductionAgent implementation
‚îÇ   ‚îú‚îÄ‚îÄ deduction/              # ‚úÖ Logical validation and reasoning
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ agent.py           # ‚úÖ DeductionAgent implementation
‚îÇ   ‚îî‚îÄ‚îÄ induction/              # ‚úÖ Pattern synthesis and generalization
‚îÇ       ‚îî‚îÄ‚îÄ agent.py           # ‚úÖ InductionAgent implementation
‚îú‚îÄ‚îÄ layers/                      # ‚úÖ Evaluation infrastructure
‚îÇ   ‚îú‚îÄ‚îÄ quantitative/           # ‚úÖ Automated metrics (diversity, quality)
‚îÇ   ‚îú‚îÄ‚îÄ llm_judges/             # ‚úÖ AI-powered evaluation
‚îÇ   ‚îî‚îÄ‚îÄ human_eval/             # ‚úÖ Human assessment interface
‚îú‚îÄ‚îÄ examples/                    # ‚úÖ Usage demonstrations
‚îÇ   ‚îú‚îÄ‚îÄ qadi_demo.py            # ‚úÖ Complete QADI system demo
‚îÇ   ‚îî‚îÄ‚îÄ basic_usage.py          # ‚úÖ Basic evaluation examples
‚îú‚îÄ‚îÄ tests/                       # ‚úÖ Comprehensive test suite
‚îÇ   ‚îú‚îÄ‚îÄ test_qadi_system.py     # ‚úÖ QADI agents and orchestration tests
‚îÇ   ‚îî‚îÄ‚îÄ unit/                   # ‚úÖ Unit tests for components
‚îî‚îÄ‚îÄ cli.py                       # ‚úÖ Command-line interface
```

### üöß Future Enhancements (Genetic Evolution)
```
src/mad_spark_alt/
‚îú‚îÄ‚îÄ evolution/                   # üöß Genetic algorithm implementation
‚îÇ   ‚îú‚îÄ‚îÄ genetic_algorithm.py    # üöß Population evolution engine
‚îÇ   ‚îú‚îÄ‚îÄ fitness.py              # üöß Idea fitness evaluation
‚îÇ   ‚îî‚îÄ‚îÄ operators.py            # üöß Crossover, mutation, selection
‚îú‚îÄ‚îÄ collaboration/               # üöß Human-AI interaction
‚îÇ   ‚îú‚îÄ‚îÄ interface.py            # üöß Interactive ideation sessions
‚îÇ   ‚îî‚îÄ‚îÄ feedback.py             # üöß Human feedback integration
‚îî‚îÄ‚îÄ web/                         # üöß Web interface (optional)
    ‚îú‚îÄ‚îÄ api.py                  # üöß REST API for remote access
    ‚îî‚îÄ‚îÄ dashboard.py            # üöß Real-time monitoring dashboard
```

### Key Classes

**Core Interfaces** (`core/interfaces.py`):
- `ThinkingAgentInterface`: Abstract base for all thinking agents ‚úÖ
- `EvaluatorInterface`: Abstract base for all evaluators ‚úÖ
- `IdeaGenerationRequest`: Input data structure for idea generation ‚úÖ
- `IdeaGenerationResult`: Output data structure from agents ‚úÖ
- `GeneratedIdea`: Rich idea representation with metadata ‚úÖ
- `ThinkingMethod`: Enum (QUESTIONING, ABDUCTION, DEDUCTION, INDUCTION) ‚úÖ
- `EvaluationRequest`: Input data structure for evaluation ‚úÖ
- `EvaluationResult`: Output data structure from evaluators ‚úÖ
- `ModelOutput`: Represents AI-generated content to evaluate ‚úÖ

**QADI Orchestration** (`core/orchestrator.py`):
- `QADIOrchestrator`: Coordinates multi-phase thinking cycles ‚úÖ
- `QADICycleResult`: Complete cycle result with phase breakdowns ‚úÖ
- Handles sequential and parallel agent processing ‚úÖ
- Enhanced context building between phases ‚úÖ

**Registry System** (`core/registry.py`):
- `ThinkingAgentRegistry`: Dynamic agent registration and management ‚úÖ
- `EvaluatorRegistry`: Dynamic evaluator registration ‚úÖ 
- `UnifiedRegistry`: Seamless integration of both systems ‚úÖ
- Method-based agent retrieval and discovery ‚úÖ

**Creativity Evaluation** (`core/evaluator.py`):
- `CreativityEvaluator`: Coordinates evaluation across all layers ‚úÖ
- Handles async execution, result aggregation, scoring ‚úÖ

### Layer Implementations

**Quantitative Layer**:
- `DiversityEvaluator`: Distinct-N, semantic uniqueness, lexical diversity
- `QualityEvaluator`: Fluency, grammar, readability, coherence

**LLM Judge Layer**:
- `CreativityLLMJudge`: Single AI model evaluation
- `CreativityJury`: Multi-judge consensus with disagreement detection
- Supports GPT-4, Claude-3, Gemini models
- Cost tracking and budget management

## Development Patterns

### Adding New Evaluators
Implement `EvaluatorInterface` and register:

```python
from typing import Any, Dict, List
from mad_spark_alt.core import EvaluatorInterface, register_evaluator
from mad_spark_alt.core.interfaces import EvaluationLayer, EvaluationRequest, EvaluationResult, OutputType

class MyEvaluator(EvaluatorInterface):
    @property
    def name(self) -> str:
        return "my_evaluator"
    
    @property
    def layer(self) -> EvaluationLayer:
        return EvaluationLayer.QUANTITATIVE
    
    @property
    def supported_output_types(self) -> List[OutputType]:
        return [OutputType.TEXT]
    
    async def evaluate(self, request: EvaluationRequest) -> List[EvaluationResult]:
        # Implementation here
        pass
    
    def validate_config(self, config: Dict[str, Any]) -> bool:
        return True

register_evaluator(MyEvaluator)
```

### Async Pattern
All evaluators use async/await for I/O operations:
- LLM API calls are concurrent where possible
- Batch processing for efficiency
- Proper error handling and timeouts

### Data Flow
1. `EvaluationRequest` created with `ModelOutput` list
2. `CreativityEvaluator.evaluate()` routes to appropriate layers
3. Each evaluator returns `EvaluationResult` list  
4. Results aggregated into `EvaluationSummary`
5. Overall creativity score calculated

### Error Handling
- Graceful fallbacks for missing API keys
- Individual evaluator failures don't crash entire evaluation
- Structured error responses with context

## Testing Strategy

### Test Structure
```
tests/
‚îú‚îÄ‚îÄ unit/                   # Unit tests for individual components
‚îÇ   ‚îú‚îÄ‚îÄ test_core.py       # Core functionality tests
‚îÇ   ‚îî‚îÄ‚îÄ test_llm_judges.py # LLM judge specific tests
‚îî‚îÄ‚îÄ integration/           # End-to-end integration tests
```

### Mock Strategy
- Mock external API calls to avoid costs during testing
- Test data includes various creativity dimensions
- Separate tests for different output types (text, code)

### Test Execution
Run tests without external dependencies by default. Use environment variables to enable real API testing during development.

### Quick Development Verification
```bash
# Install package first
uv pip install -e .

# Verify installation and basic functionality
uv run mad-spark --help
uv run mad-spark list-evaluators

# Test basic evaluation
uv run mad-spark evaluate "The AI dreamed of electric sheep in quantum meadows."

# Test QADI system (template-based generation)
uv run python examples/qadi_demo.py

# Test core functionality without API keys
uv run pytest tests/unit/test_core.py -v

# Run all tests
uv run pytest

# Check registry state (useful for debugging)
uv run python -c "from mad_spark_alt.core import registry; print([e.name for e in registry.get_evaluators()])"
```

## Important Implementation Notes

### Cost Management
LLM judges track token usage and costs. Budget-aware jury configurations prevent runaway expenses.

### Multi-Judge Consensus
The jury system uses voting mechanisms to handle disagreements between AI judges, improving evaluation reliability.

### Extensibility
The plugin architecture allows easy addition of new evaluation methods without modifying core code.

### Performance
Async execution enables parallel evaluation across multiple AI services, significantly reducing total evaluation time.